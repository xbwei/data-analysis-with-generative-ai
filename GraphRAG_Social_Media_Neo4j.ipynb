{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xbwei/data-analysis-with-generative-ai/blob/main/GraphRAG_Social_Media_Neo4j.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5a95af",
      "metadata": {
        "id": "dd5a95af"
      },
      "source": [
        "# ğŸ§  GraphRAG: Retrieval-Augmented Generation with Neo4j Knowledge Graph\n",
        "\n",
        "**Ask natural-language questions â€” get graph-powered answers.**\n",
        "\n",
        "In the previous notebook (`Social_Media_ETL_Neo4j_Python.ipynb`) we built a social media knowledge graph in Neo4j with **Users, Tweets, Hashtags, and Places**. Now we will layer **AI** on top of that graph to create a **GraphRAG** pipeline.\n",
        "\n",
        "**What is GraphRAG?**\n",
        "\n",
        "Traditional RAG retrieves relevant text chunks from a vector store. **GraphRAG** goes further â€” it combines:\n",
        "1. **Vector Search** â€” find semantically similar tweets using embeddings.\n",
        "2. **Graph Traversal** â€” follow relationships (who posted it, where, which hashtags) to enrich context.\n",
        "3. **LLM Generation** â€” pass the enriched context to a language model for a grounded answer.\n",
        "\n",
        "![GraphRAG Pipeline](https://github.com/lbsocial/data-analysis-with-generative-ai/blob/1660b372f63accb8e55ea4b439924ba764639182/image/Gemini_Generated_Image_vslfdxvslfdxvslf.png?raw=true)\n",
        "\n",
        "**What we will build:**\n",
        "| Step | Action |\n",
        "|------|--------|\n",
        "| 1 | Install dependencies |\n",
        "| 2 | Connect to Neo4j & Gemini |\n",
        "| 3 | Create vector embeddings for every Tweet |\n",
        "| 4 | Build a Neo4j Vector Index |\n",
        "| 5 | Implement pure Vector Search retrieval |\n",
        "| 6 | Implement GraphRAG retrieval (Vector + Graph Traversal) |\n",
        "| 7 | Build the full RAG pipeline with Gemini |\n",
        "| 8 | Compare Vector-only vs GraphRAG answers |\n",
        "| 9 | Interactive GraphRAG query |\n",
        "| 10 | Advanced â€” Cypher-Augmented Generation |\n",
        "| 11 | Geospatial queries & Geo-Augmented GraphRAG |---\n",
        "\n",
        "\n",
        "> **Why no LangChain?** Frameworks like LangChain provide convenient abstractions (`GraphCypherQAChain`, `Neo4jVector`, etc.) that can simplify production code. However, in this tutorial we use the **Neo4j Python driver** and **Google GenAI SDK** directly so you can see exactly how each piece works â€” embedding, vector search, graph traversal, prompt formatting, and LLM generation. Once you understand these building blocks, adopting a framework becomes much easier."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46192788",
      "metadata": {
        "id": "46192788"
      },
      "source": [
        "## ğŸ› ï¸ Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "960268cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960268cf",
        "outputId": "df592748-d8c9-4e97-b1f0-cea70cb8e1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/325.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.7/325.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m317.4/325.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m325.3/325.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install neo4j google-genai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2316f9be",
      "metadata": {
        "id": "2316f9be"
      },
      "source": [
        "## ğŸ”Œ Step 2: Connect to Neo4j & Gemini\n",
        "\n",
        "We need two connections:\n",
        "- **Neo4j** â€” our knowledge graph.\n",
        "- **Google Gemini** â€” for generating embeddings and LLM answers.\n",
        "\n",
        "> **Note:** Store your credentials as environment variables or in Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4cec4559",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cec4559",
        "outputId": "f819d090-7cc0-4c94-c188-334050989205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Connected to Neo4j\n",
            "âœ… Connected to Gemini (Gemini 2.0 Flash)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from neo4j import GraphDatabase\n",
        "from google import genai\n",
        "\n",
        "# â”€â”€ Neo4j Credentials (from Colab Secrets) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "NEO4J_URI = userdata.get('NEO4J_URI')\n",
        "NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n",
        "NEO4J_AUTH = (\"neo4j\", NEO4J_PASSWORD)\n",
        "\n",
        "# â”€â”€ Gemini Client (from Colab Secrets) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# â”€â”€ Verify Neo4j Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)\n",
        "driver.verify_connectivity()\n",
        "print(\"âœ… Connected to Neo4j\")\n",
        "\n",
        "# â”€â”€ Verify Gemini Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "test = client.models.get(model=\"gemini-2.0-flash\")\n",
        "print(f\"âœ… Connected to Gemini ({test.display_name})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cca90c6f",
      "metadata": {
        "id": "cca90c6f"
      },
      "source": [
        "## ğŸ“Š Step 2b: Verify the Graph Data\n",
        "\n",
        "Let's confirm the graph we built in the ETL notebook is ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "97025886",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97025886",
        "outputId": "36ee9e70-71ad-45f3-e626-738e35fa5da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neo4j.notifications:Received notification from DBMS server: <GqlStatusObject gql_status='01N00', status_description='warn: feature deprecated. CALL subquery without a variable scope clause is deprecated. Use CALL () { ... }', position=<SummaryInputPosition line=2, column=9, offset=9>, raw_classification='DEPRECATION', classification=<NotificationClassification.DEPRECATION: 'DEPRECATION'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'DEPRECATION', '_severity': 'WARNING', '_position': {'offset': 9, 'line': 2, 'column': 9}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: \"\\n        CALL {\\n            MATCH (t:Tweet) RETURN 'Tweet' AS label, count(t) AS count\\n            UNION ALL\\n            MATCH (u:User)  RETURN 'User'  AS label, count(u) AS count\\n            UNION ALL\\n            MATCH (h:Hashtag) RETURN 'Hashtag' AS label, count(h) AS count\\n            UNION ALL\\n            MATCH (p:Place) RETURN 'Place' AS label, count(p) AS count\\n        }\\n        RETURN label, count\\n    \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”€â”€ Graph Node Summary â”€â”€\n",
            "       Tweet: 100\n",
            "        User: 5\n",
            "     Hashtag: 4\n",
            "       Place: 5\n",
            "\n",
            "â”€â”€ Sample Tweets â”€â”€\n",
            "  @Eve_Sec from London, UK\n",
            "    \"Pandas and NumPy are essential tools for any data engineer. #Python #Cloud #AI...\"\n",
            "    Tags: ['python', 'cloud', 'ai', 'python', 'cloud', 'ai']\n",
            "\n",
            "  @Charlie_AI from London, UK\n",
            "    \"The new Large Language Models are hallucinating less and reasoning more. #AI #Ne...\"\n",
            "    Tags: ['ai', 'neo4j']\n",
            "\n",
            "  @Dave_Dev from London, UK\n",
            "    \"Automating my daily workflows with a simple Python script. #Python #Cloud #Neo4j...\"\n",
            "    Tags: ['python', 'cloud', 'neo4j']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick health check â€” count nodes by label\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        CALL {\n",
        "            MATCH (t:Tweet) RETURN 'Tweet' AS label, count(t) AS count\n",
        "            UNION ALL\n",
        "            MATCH (u:User)  RETURN 'User'  AS label, count(u) AS count\n",
        "            UNION ALL\n",
        "            MATCH (h:Hashtag) RETURN 'Hashtag' AS label, count(h) AS count\n",
        "            UNION ALL\n",
        "            MATCH (p:Place) RETURN 'Place' AS label, count(p) AS count\n",
        "        }\n",
        "        RETURN label, count\n",
        "    \"\"\")\n",
        "    print(\"â”€â”€ Graph Node Summary â”€â”€\")\n",
        "    for record in result:\n",
        "        print(f\"  {record['label']:>10s}: {record['count']}\")\n",
        "\n",
        "# Sample one tweet with all its relationships\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (u:User)-[:POSTED]->(t:Tweet)-[:LOCATED_AT]->(p:Place)\n",
        "        OPTIONAL MATCH (t)-[:TAGGED_WITH]->(h:Hashtag)\n",
        "        RETURN u.username AS user, t.text AS text,\n",
        "               p.name AS place, collect(h.name) AS hashtags\n",
        "        LIMIT 3\n",
        "    \"\"\")\n",
        "    print(\"\\nâ”€â”€ Sample Tweets â”€â”€\")\n",
        "    for record in result:\n",
        "        print(f\"  @{record['user']} from {record['place']}\")\n",
        "        print(f\"    \\\"{record['text'][:80]}...\\\"\")\n",
        "        print(f\"    Tags: {record['hashtags']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7d9a78d",
      "metadata": {
        "id": "c7d9a78d"
      },
      "source": [
        "---\n",
        "## ğŸ§¬ Step 3: Generate Vector Embeddings for Tweets\n",
        "\n",
        "We use Gemini's `gemini-embedding-001` model to convert every tweet's text into a **3072-dimensional vector**. This vector captures the *semantic meaning* of the text.\n",
        "\n",
        "We then write each embedding back to the `Tweet` node as a property called `embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d545a106",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d545a106",
        "outputId": "e02204d3-831f-410f-d132-5e7b10c2cc46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Found 100 tweets to embed.\n",
            "  âœ… Embedded batch 1/5\n",
            "  âœ… Embedded batch 2/5\n",
            "  âœ… Embedded batch 3/5\n",
            "  âœ… Embedded batch 4/5\n",
            "  âœ… Embedded batch 5/5\n",
            "\n",
            "ğŸ‰ All tweet embeddings stored in Neo4j!\n"
          ]
        }
      ],
      "source": [
        "EMBEDDING_MODEL = \"models/gemini-embedding-001\"\n",
        "EMBEDDING_DIM = 3072\n",
        "\n",
        "\n",
        "def get_embeddings(texts: list[str]) -> list[list[float]]:\n",
        "    \"\"\"Get Gemini embeddings for a batch of texts.\"\"\"\n",
        "    response = client.models.embed_content(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        contents=texts,\n",
        "    )\n",
        "    return [emb.values for emb in response.embeddings]\n",
        "\n",
        "\n",
        "# â”€â”€ Fetch all tweet texts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"MATCH (t:Tweet) RETURN t.id AS id, t.text AS text\")\n",
        "    tweets = [(record[\"id\"], record[\"text\"]) for record in result]\n",
        "\n",
        "print(f\"ğŸ“„ Found {len(tweets)} tweets to embed.\")\n",
        "\n",
        "# â”€â”€ Batch embed (20 tweets at a time) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BATCH_SIZE = 20\n",
        "for i in range(0, len(tweets), BATCH_SIZE):\n",
        "    batch = tweets[i : i + BATCH_SIZE]\n",
        "    ids = [t[0] for t in batch]\n",
        "    texts = [t[1] for t in batch]\n",
        "\n",
        "    embeddings = get_embeddings(texts)\n",
        "\n",
        "    # Write embeddings back to Neo4j\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        session.run(\n",
        "            \"\"\"\n",
        "            UNWIND $data AS row\n",
        "            MATCH (t:Tweet {id: row.id})\n",
        "            SET t.embedding = row.embedding\n",
        "            \"\"\",\n",
        "            data=[{\"id\": id_, \"embedding\": emb} for id_, emb in zip(ids, embeddings)],\n",
        "        )\n",
        "\n",
        "    print(f\"  âœ… Embedded batch {i // BATCH_SIZE + 1}/{(len(tweets) - 1) // BATCH_SIZE + 1}\")\n",
        "\n",
        "\n",
        "print(\"\\nğŸ‰ All tweet embeddings stored in Neo4j!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fc570a",
      "metadata": {
        "id": "47fc570a"
      },
      "source": [
        "---\n",
        "## ğŸ—‚ï¸ Step 4: Create a Neo4j Vector Index\n",
        "\n",
        "A **vector index** lets Neo4j perform fast approximate-nearest-neighbor (ANN) search on the embedding property. This is the foundation of our retrieval step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "25bb5bb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25bb5bb8",
        "outputId": "027a5a44-6fa6-4afb-b804-5c17cd21185c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to drop existing index 'tweet_embeddings' if it exists...\n",
            "âœ… Index 'tweet_embeddings' dropped (if it existed).\n",
            "âœ… Vector index 'tweet_embeddings' created with 3072 dimensions.\n",
            "  Index: tweet_embeddings\n",
            "  State: POPULATING\n",
            "  Index configuration properties not found.\n"
          ]
        }
      ],
      "source": [
        "INDEX_NAME = \"tweet_embeddings\"\n",
        "\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    # Ensure the index is dropped idempotently\n",
        "    print(f\"Attempting to drop existing index '{INDEX_NAME}' if it exists...\")\n",
        "    session.run(f\"DROP INDEX {INDEX_NAME} IF EXISTS\")\n",
        "    print(f\"âœ… Index '{INDEX_NAME}' dropped (if it existed).\")\n",
        "\n",
        "    # Create the vector index with the correct dimensions\n",
        "    session.run(f\"\"\"\n",
        "        CREATE VECTOR INDEX {INDEX_NAME}\n",
        "        FOR (t:Tweet)\n",
        "        ON (t.embedding)\n",
        "        OPTIONS {{\n",
        "            indexConfig: {{\n",
        "                `vector.dimensions`: {EMBEDDING_DIM},\n",
        "                `vector.similarity_function`: 'cosine'\n",
        "            }}\n",
        "        }}\n",
        "    \"\"\")\n",
        "    print(f\"âœ… Vector index '{INDEX_NAME}' created with {EMBEDDING_DIM} dimensions.\")\n",
        "\n",
        "# Verify index details, specifically dimensions\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    # Run SHOW VECTOR INDEXES and then filter in Python\n",
        "    result = session.run(\"SHOW VECTOR INDEXES\")\n",
        "    found_index = False\n",
        "    for record in result:\n",
        "        if record['name'] == INDEX_NAME:\n",
        "            found_index = True\n",
        "            print(f\"  Index: {record['name']}\")\n",
        "            print(f\"  State: {record['state']}\")\n",
        "            # Extract dimensions from the 'properties' map\n",
        "            if 'properties' in record and 'indexConfig' in record['properties']:\n",
        "                config = record['properties']['indexConfig']\n",
        "                dimensions = config.get('vector.dimensions')\n",
        "                if dimensions:\n",
        "                    print(f\"  Actual Dimensions: {dimensions}\")\n",
        "                else:\n",
        "                    print(\"  Dimensions not found in index properties.\")\n",
        "            else:\n",
        "                print(\"  Index configuration properties not found.\")\n",
        "            break\n",
        "    if not found_index:\n",
        "        print(f\"âŒ Index '{INDEX_NAME}' not found after creation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae6b082",
      "metadata": {
        "id": "fae6b082"
      },
      "source": [
        "---\n",
        "## ğŸ” Step 5: Pure Vector Search (Baseline)\n",
        "\n",
        "First, let's implement a plain **vector similarity search**. We embed the user's question, then find the `k` most similar tweets. This is what a traditional RAG system does â€” **no graph knowledge** is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cd877d2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd877d2a",
        "outputId": "3e2d4e5b-016b-48ff-e99a-fda647dccab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Vector Search Results for: 'What are people saying about graph databases?'\n",
            "\n",
            "  1. [score=0.8845] Graph databases are game changers for handling complex relationships. #Neo4j\n",
            "  2. [score=0.8803] Graph databases are game changers for handling complex relationships. #Neo4j #AI\n",
            "  3. [score=0.8740] Graph databases are game changers for handling complex relationships. #Neo4j #Python\n",
            "  4. [score=0.8733] Graph databases are game changers for handling complex relationships. #Neo4j #AI #Python\n",
            "  5. [score=0.8547] Relational DBs struggle with joins, but graphs handle them naturally. #Neo4j\n"
          ]
        }
      ],
      "source": [
        "def vector_search(question: str, k: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Plain vector search â€” returns the top-k most similar tweets.\n",
        "    No graph traversal.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embeddings([question])[0]\n",
        "\n",
        "    cypher = \"\"\"\n",
        "        CALL db.index.vector.queryNodes($index, $k, $embedding)\n",
        "        YIELD node AS tweet, score\n",
        "        RETURN tweet.id    AS id,\n",
        "               tweet.text  AS text,\n",
        "               tweet.likes AS likes,\n",
        "               score\n",
        "        ORDER BY score DESC\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(cypher, index=INDEX_NAME, k=k, embedding=q_embedding)\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "# â”€â”€ Test it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "question = \"What are people saying about graph databases?\"\n",
        "results = vector_search(question)\n",
        "\n",
        "print(f\"ğŸ” Vector Search Results for: '{question}'\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"  {i}. [score={r['score']:.4f}] {r['text'][:90]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f5b073e",
      "metadata": {
        "id": "7f5b073e"
      },
      "source": [
        "---\n",
        "## ğŸ•¸ï¸ Step 6: GraphRAG Retrieval (Vector + Graph Traversal)\n",
        "\n",
        "This is the key innovation. After finding similar tweets via vector search, we **traverse the graph** to collect rich context:\n",
        "\n",
        "| Traversal | What we get |\n",
        "|-----------|-------------|\n",
        "| `Tweet â† POSTED â† User` | Who wrote it? How many followers? |\n",
        "| `Tweet â†’ LOCATED_AT â†’ Place` | Where was it posted? |\n",
        "| `Tweet â†’ TAGGED_WITH â†’ Hashtag` | What topics is it about? |\n",
        "| `User â†’ POSTED â†’ OtherTweets` | What else has this user said? (context expansion) |\n",
        "| `Hashtag â† TAGGED_WITH â† OtherTweets` | What else is tagged with the same topics? |\n",
        "\n",
        "This gives the LLM **much richer context** than bare text chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "51b3d6f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51b3d6f7",
        "outputId": "e4ad848f-9ab3-45e4-de4b-6823d66cc877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ•¸ï¸ GraphRAG Results for: 'What are people saying about graph databases?'\n",
            "\n",
            "  1. [score=0.8845] @Alice_Data (33440 followers)\n",
            "     ğŸ“ London, UK, GB\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j...\"\n",
            "     ğŸ·ï¸  Tags: ['neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  2. [score=0.8803] @Dave_Dev (40016 followers)\n",
            "     ğŸ“ San Francisco, CA, US\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j #AI...\"\n",
            "     ğŸ·ï¸  Tags: ['ai', 'neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  3. [score=0.8740] @Alice_Data (33440 followers)\n",
            "     ğŸ“ Tokyo, JP, JP\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j #Py...\"\n",
            "     ğŸ·ï¸  Tags: ['python', 'neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  4. [score=0.8733] @Alice_Data (33440 followers)\n",
            "     ğŸ“ Paris, FR, FR\n",
            "     ğŸ’¬ \"Graph databases are game changers for handling complex relationships. #Neo4j #AI...\"\n",
            "     ğŸ·ï¸  Tags: ['python', 'ai', 'neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n",
            "  5. [score=0.8547] @Eve_Sec (25197 followers)\n",
            "     ğŸ“ Tokyo, JP, JP\n",
            "     ğŸ’¬ \"Relational DBs struggle with joins, but graphs handle them naturally. #Neo4j...\"\n",
            "     ğŸ·ï¸  Tags: ['neo4j']\n",
            "     ğŸ”— Related: \"Pandas and NumPy are essential tools for any data engineer. ...\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def graph_rag_search(question: str, k: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    GraphRAG retrieval â€” vector search + graph traversal.\n",
        "    Returns enriched context for each matching tweet.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embeddings([question])[0]\n",
        "\n",
        "    cypher = \"\"\"\n",
        "        // â”€â”€ 1. Vector Search: find semantically similar tweets â”€â”€\n",
        "        CALL db.index.vector.queryNodes($index, $k, $embedding)\n",
        "        YIELD node AS tweet, score\n",
        "\n",
        "        // â”€â”€ 2. Graph Traversal: enrich with relationships â”€â”€\n",
        "        // Get the author\n",
        "        MATCH (author:User)-[:POSTED]->(tweet)\n",
        "\n",
        "        // Get the location\n",
        "        OPTIONAL MATCH (tweet)-[:LOCATED_AT]->(place:Place)\n",
        "\n",
        "        // Get all hashtags on this tweet\n",
        "        OPTIONAL MATCH (tweet)-[:TAGGED_WITH]->(hashtag:Hashtag)\n",
        "\n",
        "        // â”€â”€ 3. Context Expansion: other tweets by same author â”€â”€\n",
        "        OPTIONAL MATCH (author)-[:POSTED]->(other_tweet:Tweet)\n",
        "        WHERE other_tweet.id <> tweet.id\n",
        "\n",
        "        // â”€â”€ 4. Context Expansion: co-occurring tweets via hashtags â”€â”€\n",
        "        OPTIONAL MATCH (hashtag)<-[:TAGGED_WITH]-(related_tweet:Tweet)\n",
        "        WHERE related_tweet.id <> tweet.id\n",
        "\n",
        "        RETURN tweet.id                        AS tweet_id,\n",
        "               tweet.text                      AS text,\n",
        "               tweet.likes                     AS likes,\n",
        "               tweet.retweets                  AS retweets,\n",
        "               score,\n",
        "\n",
        "               // Author context\n",
        "               author.username                 AS author,\n",
        "               author.followers                AS author_followers,\n",
        "\n",
        "               // Location context\n",
        "               place.name                      AS location,\n",
        "               place.country                   AS country,\n",
        "\n",
        "               // Hashtag context\n",
        "               collect(DISTINCT hashtag.name)   AS hashtags,\n",
        "\n",
        "               // Expanded context\n",
        "               collect(DISTINCT other_tweet.text)[0..3]   AS author_other_tweets,\n",
        "               collect(DISTINCT related_tweet.text)[0..3] AS related_tweets_via_hashtag\n",
        "\n",
        "        ORDER BY score DESC\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(cypher, index=INDEX_NAME, k=k, embedding=q_embedding)\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "# â”€â”€ Test it â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "question = \"What are people saying about graph databases?\"\n",
        "results = graph_rag_search(question)\n",
        "\n",
        "print(f\"ğŸ•¸ï¸ GraphRAG Results for: '{question}'\\n\")\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"  {i}. [score={r['score']:.4f}] @{r['author']} ({r['author_followers']} followers)\")\n",
        "    print(f\"     ğŸ“ {r['location']}, {r['country']}\")\n",
        "    print(f\"     ğŸ’¬ \\\"{r['text'][:80]}...\\\"\")\n",
        "    print(f\"     ğŸ·ï¸  Tags: {r['hashtags']}\")\n",
        "    if r['related_tweets_via_hashtag']:\n",
        "        print(f\"     ğŸ”— Related: \\\"{r['related_tweets_via_hashtag'][0][:60]}...\\\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc78870a",
      "metadata": {
        "id": "dc78870a"
      },
      "source": [
        "---\n",
        "## ğŸ¤– Step 7: Build the Full RAG Pipeline with Gemini\n",
        "\n",
        "Now we connect everything: **retrieve** enriched context from the graph, **format** it into a prompt, and **generate** an answer with Gemini.\n",
        "\n",
        "We build two pipelines side-by-side:\n",
        "- `rag_answer()` â€” plain vector retrieval + Gemini\n",
        "- `graph_rag_answer()` â€” GraphRAG retrieval + Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "40af5918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40af5918",
        "outputId": "89fda0d7-5368-42be-9b45-4093c5f9c299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… RAG pipelines ready.\n"
          ]
        }
      ],
      "source": [
        "LLM_MODEL = \"gemini-2.0-flash\"\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a social media analyst assistant.\n",
        "Answer the user's question based ONLY on the retrieved context below.\n",
        "If the context doesn't contain enough information, say so.\n",
        "Always cite specific tweets, users, or locations when relevant.\n",
        "Be concise but thorough.\"\"\"\n",
        "\n",
        "\n",
        "def format_vector_context(results: list[dict]) -> str:\n",
        "    \"\"\"Format plain vector search results into a text block.\"\"\"\n",
        "    lines = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        lines.append(f\"Tweet {i} (similarity={r['score']:.3f}, likes={r['likes']}):\\n  \\\"{r['text']}\\\"\")\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "def format_graph_context(results: list[dict]) -> str:\n",
        "    \"\"\"Format GraphRAG results into a rich context block.\"\"\"\n",
        "    lines = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        block = f\"\"\"Tweet {i} (similarity={r['score']:.3f}):\n",
        "  Text: \"{r['text']}\"\n",
        "  Author: @{r['author']} ({r['author_followers']} followers)\n",
        "  Location: {r['location']}, {r['country']}\n",
        "  Engagement: {r['likes']} likes, {r['retweets']} retweets\n",
        "  Hashtags: {', '.join(r['hashtags'])}\"\"\"\n",
        "        if r.get('author_other_tweets'):\n",
        "            block += f\"\\n  Other tweets by @{r['author']}:\"\n",
        "            for t in r['author_other_tweets']:\n",
        "                block += f\"\\n    - \\\"{t[:80]}...\\\"\"\n",
        "        if r.get('related_tweets_via_hashtag'):\n",
        "            block += f\"\\n  Related tweets (same hashtags):\"\n",
        "            for t in r['related_tweets_via_hashtag']:\n",
        "                block += f\"\\n    - \\\"{t[:80]}...\\\"\"\n",
        "        lines.append(block)\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "\n",
        "def rag_answer(question: str, k: int = 5) -> str:\n",
        "    \"\"\"Traditional RAG: vector search + LLM.\"\"\"\n",
        "    results = vector_search(question, k)\n",
        "    context = format_vector_context(results)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=SYSTEM_PROMPT,\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def graph_rag_answer(question: str, k: int = 5) -> str:\n",
        "    \"\"\"GraphRAG: vector search + graph traversal + LLM.\"\"\"\n",
        "    results = graph_rag_search(question, k)\n",
        "    context = format_graph_context(results)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=SYSTEM_PROMPT,\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "print(\"âœ… RAG pipelines ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03fa285",
      "metadata": {
        "id": "a03fa285"
      },
      "source": [
        "---\n",
        "## âš–ï¸ Step 8: Compare Vector-Only RAG vs GraphRAG\n",
        "\n",
        "Let's ask the same questions and compare the quality of answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "af9459ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "af9459ed",
        "outputId": "7bd69922-5924-4ca2-8d97-260031ffe501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "â“ Question: What are people saying about graph databases and who are the most active users talking about them?\n",
            "================================================================================\n",
            "\n",
            "ğŸ“„ â”€â”€ TRADITIONAL RAG (Vector Only) â”€â”€\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 19.059603118s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '19s'}]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1786007839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸ“„ â”€â”€ TRADITIONAL RAG (Vector Only) â”€â”€\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrag_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸ•¸ï¸ â”€â”€ GRAPHRAG (Vector + Graph Traversal) â”€â”€\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-778890496.py\u001b[0m in \u001b[0;36mrag_answer\u001b[0;34m(question, k)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_vector_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     response = client.models.generate_content(\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLM_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Context:\\n{context}\\n\\nQuestion: {question}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5225\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5227\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5228\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5229\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4007\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4009\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4010\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4011\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[0;32m-> 1386\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m     response_body = (\n\u001b[1;32m   1388\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1197\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m       )\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       return HttpResponse(\n\u001b[1;32m   1201\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 19.059603118s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model':..."
          ]
        }
      ],
      "source": [
        "comparison_questions = [\n",
        "    \"What are people saying about graph databases and who are the most active users talking about them?\",\n",
        "    \"Which cities generate the most discussion about AI and cloud computing?\",\n",
        "    \"Are there any users who tweet about both Python and Neo4j? What are they saying?\",\n",
        "]\n",
        "\n",
        "for question in comparison_questions:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"â“ Question: {question}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\nğŸ“„ â”€â”€ TRADITIONAL RAG (Vector Only) â”€â”€\")\n",
        "    print(rag_answer(question))\n",
        "\n",
        "    print(\"\\nğŸ•¸ï¸ â”€â”€ GRAPHRAG (Vector + Graph Traversal) â”€â”€\")\n",
        "    print(graph_rag_answer(question))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42529791",
      "metadata": {
        "id": "42529791"
      },
      "source": [
        "---\n",
        "## ğŸ§ª Step 9: Interactive GraphRAG Query\n",
        "\n",
        "Try your own questions below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "56517c87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "56517c87",
        "outputId": "f92c1960-11fb-4f46-feac-7e4605d4732b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â“ What topics are trending in San Francisco?\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 12.599163909s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '12s'}]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1006006856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"â“ {your_question}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_rag_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myour_question\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-778890496.py\u001b[0m in \u001b[0;36mgraph_rag_answer\u001b[0;34m(question, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_graph_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     response = client.models.generate_content(\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLM_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Context:\\n{context}\\n\\nQuestion: {question}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5225\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5227\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5228\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5229\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4007\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4009\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4010\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4011\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[0;32m-> 1386\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m     response_body = (\n\u001b[1;32m   1388\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1197\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m       )\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       return HttpResponse(\n\u001b[1;32m   1201\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\nPlease retry in 12.599163909s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model':..."
          ]
        }
      ],
      "source": [
        "# â”€â”€ Change this question to anything you want â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "your_question = \"What topics are trending in San Francisco?\"\n",
        "\n",
        "print(f\"â“ {your_question}\\n\")\n",
        "print(graph_rag_answer(your_question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755b4fcf",
      "metadata": {
        "id": "755b4fcf"
      },
      "source": [
        "---\n",
        "## ğŸ”¬ Step 10: Advanced â€” Cypher-Augmented Generation\n",
        "\n",
        "For structured analytical questions (\"How many tweets per city?\"), we can let the LLM **generate Cypher queries** directly. This combines the power of graph queries with natural language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3b90f060",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b90f060",
        "outputId": "0882d3f7-c021-4abf-c9fe-73b1bac7e7f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Cypher-Augmented Generation ready.\n"
          ]
        }
      ],
      "source": [
        "CYPHER_SYSTEM_PROMPT = \"\"\"You are a Neo4j Cypher expert. Given the user's question, generate a Cypher query to answer it.\n",
        "\n",
        "The graph schema is:\n",
        "- (:User {id, username, name, followers, following, tweet_count})\n",
        "    -[:POSTED]->(:Tweet {id, text, created_at, likes, retweets, replies, location, embedding})\n",
        "- (:Tweet)-[:LOCATED_AT]->(:Place {name, country, location})\n",
        "- (:Tweet)-[:TAGGED_WITH]->(:Hashtag {name})\n",
        "\n",
        "Rules:\n",
        "- Return ONLY raw Cypher (no markdown, no explanation, no code fences).\n",
        "- Use LIMIT 10 unless the user asks for more.\n",
        "- Use meaningful aliases in RETURN clauses.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def cypher_rag_answer(question: str) -> str:\n",
        "    \"\"\"Let the LLM generate a Cypher query, run it, then summarize results.\"\"\"\n",
        "\n",
        "    # Step 1: Generate Cypher\n",
        "    cypher_response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=question,\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=CYPHER_SYSTEM_PROMPT,\n",
        "            temperature=0.0,\n",
        "        ),\n",
        "    )\n",
        "    cypher_query = cypher_response.text.strip()\n",
        "    print(f\"ğŸ”§ Generated Cypher:\\n{cypher_query}\\n\")\n",
        "\n",
        "    # Step 2: Execute the Cypher query\n",
        "    try:\n",
        "        with driver.session(database=\"neo4j\") as session:\n",
        "            result = session.run(cypher_query)\n",
        "            records = [dict(record) for record in result]\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Cypher execution error: {e}\"\n",
        "\n",
        "    if not records:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    # Step 3: Summarize with LLM\n",
        "    import json\n",
        "    data_str = json.dumps(records, indent=2, default=str)\n",
        "\n",
        "    summary_response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Question: {question}\\n\\nQuery results:\\n{data_str}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=\"Summarize the following database query results in a clear, human-readable way. Use bullet points or a table if appropriate.\",\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return summary_response.text\n",
        "\n",
        "\n",
        "print(\"âœ… Cypher-Augmented Generation ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9220b7b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "9220b7b1",
        "outputId": "0c1b2ffe-aba2-4553-85aa-02821f7b0190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "â“ Which user has the most followers and what do they tweet about?\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 4.092893173s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '4s'}]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1713756367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"â“ {q}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcypher_rag_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1208960688.py\u001b[0m in \u001b[0;36mcypher_rag_answer\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Step 1: Generate Cypher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     cypher_response = client.models.generate_content(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLM_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5225\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m       \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5227\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   5228\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparsed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5229\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4007\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_unserializable_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4009\u001b[0;31m     response = self._api_client.request(\n\u001b[0m\u001b[1;32m   4010\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4011\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m     )\n\u001b[0;32m-> 1386\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m     response_body = (\n\u001b[1;32m   1388\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_stream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, http_options, stream)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[no-any-return]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m   async def _async_request_once(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mexc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0mretry_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_error_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mretry_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_attempt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request_once\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m   1197\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m       )\n\u001b[0;32m-> 1199\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m       return HttpResponse(\n\u001b[1;32m   1201\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_segments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(cls, status_code, response_json, response)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 4.092893173s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global..."
          ]
        }
      ],
      "source": [
        "# â”€â”€ Analytical questions that benefit from Cypher â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "analytical_questions = [\n",
        "    \"Which user has the most followers and what do they tweet about?\",\n",
        "    \"How many tweets were posted from each city?\",\n",
        "    \"Which hashtags are most frequently used together?\",\n",
        "]\n",
        "\n",
        "for q in analytical_questions:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"â“ {q}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(cypher_rag_answer(q))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf0bbaf",
      "metadata": {
        "id": "aaf0bbaf"
      },
      "source": [
        "---\n",
        "## ğŸŒ Step 11: Geospatial Queries â€” Find Tweets Near a Location\n",
        "\n",
        "Every Tweet and Place node in our graph has a `point()` property storing longitude/latitude. Neo4j's built-in `point.distance()` function lets us find tweets **within a radius** of any coordinate â€” no external GIS tools needed.\n",
        "\n",
        "**Use cases:**\n",
        "- \"What are people tweeting about near Times Square?\"\n",
        "- \"Which users are active within 50 km of Tokyo?\"\n",
        "- \"What topics trend in a geographic cluster?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c61d32de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c61d32de",
        "outputId": "06388b36-6315-4893-f5f4-4a7d5b7e6d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Tweets near San Francisco, CA (within 50 km):\n",
            "\n",
            "  1. @Charlie_AI â€” 1.23 km away\n",
            "     \"Deploying microservices to the edge for lower latency. #Cloud #AI...\"\n",
            "     ğŸ·ï¸ ['cloud', 'ai']  â¤ï¸ 294 likes\n",
            "\n",
            "  2. @Eve_Sec â€” 1.82 km away\n",
            "     \"Cloud costs are getting high, need to optimize our storage buckets. #Cloud #Neo4...\"\n",
            "     ğŸ·ï¸ ['cloud', 'neo4j']  â¤ï¸ 816 likes\n",
            "\n",
            "  3. @Eve_Sec â€” 2.53 km away\n",
            "     \"Just learned how to use Cypher query language, it is so intuitive! #Neo4j...\"\n",
            "     ğŸ·ï¸ ['neo4j']  â¤ï¸ 364 likes\n",
            "\n",
            "  4. @Charlie_AI â€” 2.92 km away\n",
            "     \"Just deployed a new transformer model to the cloud. #AI #Neo4j...\"\n",
            "     ğŸ·ï¸ ['ai', 'neo4j']  â¤ï¸ 274 likes\n",
            "\n",
            "  5. @Charlie_AI â€” 3.07 km away\n",
            "     \"The new Large Language Models are hallucinating less and reasoning more. #AI...\"\n",
            "     ğŸ·ï¸ ['ai']  â¤ï¸ 291 likes\n",
            "\n",
            "  6. @Charlie_AI â€” 3.44 km away\n",
            "     \"Building a recommendation engine is much easier with nodes and edges. #Neo4j #Py...\"\n",
            "     ğŸ·ï¸ ['python', 'cloud', 'neo4j']  â¤ï¸ 882 likes\n",
            "\n",
            "  7. @Alice_Data â€” 3.58 km away\n",
            "     \"Just deployed a new transformer model to the cloud. #AI...\"\n",
            "     ğŸ·ï¸ ['ai']  â¤ï¸ 499 likes\n",
            "\n",
            "  8. @Bob_Graphs â€” 3.96 km away\n",
            "     \"I love how clean Python syntax is for data science projects. #Python...\"\n",
            "     ğŸ·ï¸ ['python']  â¤ï¸ 614 likes\n",
            "\n",
            "  9. @Dave_Dev â€” 4.17 km away\n",
            "     \"Pandas and NumPy are essential tools for any data engineer. #Python #Cloud...\"\n",
            "     ğŸ·ï¸ ['python', 'cloud']  â¤ï¸ 493 likes\n",
            "\n",
            "  10. @Charlie_AI â€” 4.31 km away\n",
            "     \"Automating my daily workflows with a simple Python script. #Python #AI...\"\n",
            "     ğŸ·ï¸ ['python', 'ai']  â¤ï¸ 186 likes\n",
            "\n",
            "------------------------------------------------------------\n",
            "ğŸ“ Tweets near Tokyo, JP (within 30 km):\n",
            "\n",
            "  1. @Charlie_AI â€” 0.68 km away\n",
            "     \"Thinking about the ethics of autonomous agents in production. #AI #Cloud #Python...\"\n",
            "     ğŸ·ï¸ ['python', 'cloud', 'ai']  â¤ï¸ 83 likes\n",
            "\n",
            "  2. @Dave_Dev â€” 1.28 km away\n",
            "     \"Relational DBs struggle with joins, but graphs handle them naturally. #Neo4j #AI...\"\n",
            "     ğŸ·ï¸ ['cloud', 'ai', 'neo4j']  â¤ï¸ 598 likes\n",
            "\n",
            "  3. @Bob_Graphs â€” 1.69 km away\n",
            "     \"The new Large Language Models are hallucinating less and reasoning more. #AI #Ne...\"\n",
            "     ğŸ·ï¸ ['cloud', 'ai', 'neo4j']  â¤ï¸ 292 likes\n",
            "\n",
            "  4. @Alice_Data â€” 1.82 km away\n",
            "     \"Just learned how to use Cypher query language, it is so intuitive! #Neo4j #AI...\"\n",
            "     ğŸ·ï¸ ['ai', 'neo4j']  â¤ï¸ 340 likes\n",
            "\n",
            "  5. @Charlie_AI â€” 1.86 km away\n",
            "     \"Generative AI is transforming how we write code every day. #AI #Neo4j #Python...\"\n",
            "     ğŸ·ï¸ ['python', 'ai', 'neo4j']  â¤ï¸ 909 likes\n",
            "\n",
            "  6. @Eve_Sec â€” 2.41 km away\n",
            "     \"Pandas and NumPy are essential tools for any data engineer. #Python...\"\n",
            "     ğŸ·ï¸ ['python']  â¤ï¸ 89 likes\n",
            "\n",
            "  7. @Charlie_AI â€” 2.58 km away\n",
            "     \"Generative AI is transforming how we write code every day. #AI...\"\n",
            "     ğŸ·ï¸ ['ai']  â¤ï¸ 156 likes\n",
            "\n",
            "  8. @Dave_Dev â€” 2.67 km away\n",
            "     \"Thinking about the ethics of autonomous agents in production. #AI...\"\n",
            "     ğŸ·ï¸ ['ai']  â¤ï¸ 338 likes\n",
            "\n",
            "  9. @Eve_Sec â€” 3.18 km away\n",
            "     \"Pandas and NumPy are essential tools for any data engineer. #Python #Cloud #AI...\"\n",
            "     ğŸ·ï¸ ['python', 'cloud', 'ai']  â¤ï¸ 850 likes\n",
            "\n",
            "  10. @Eve_Sec â€” 3.28 km away\n",
            "     \"Building a recommendation engine is much easier with nodes and edges. #Neo4j #Py...\"\n",
            "     ğŸ·ï¸ ['python', 'cloud', 'neo4j']  â¤ï¸ 626 likes\n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def find_tweets_near(city_name: str, radius_km: int = 50, limit: int = 10) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Find tweets posted within `radius_km` of a city center.\n",
        "    Uses Neo4j's point.distance() for server-side geospatial filtering.\n",
        "    \"\"\"\n",
        "    cypher = \"\"\"\n",
        "        // Get the city center point\n",
        "        MATCH (p:Place {name: $city})\n",
        "        WITH p.location AS center\n",
        "\n",
        "        // Find tweets within radius\n",
        "        MATCH (u:User)-[:POSTED]->(t:Tweet)-[:LOCATED_AT]->(place:Place)\n",
        "        WHERE point.distance(t.location, center) < $radius_m\n",
        "        OPTIONAL MATCH (t)-[:TAGGED_WITH]->(h:Hashtag)\n",
        "\n",
        "        RETURN t.text                          AS text,\n",
        "               u.username                      AS author,\n",
        "               place.name                      AS place,\n",
        "               collect(DISTINCT h.name)        AS hashtags,\n",
        "               t.likes                         AS likes,\n",
        "               round(point.distance(t.location, center) / 1000.0, 2) AS distance_km\n",
        "        ORDER BY distance_km ASC\n",
        "        LIMIT $limit\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        result = session.run(cypher, city=city_name, radius_m=radius_km * 1000, limit=limit)\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "# â”€â”€ Demo: find tweets near multiple cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "geo_demos = [\n",
        "    (\"San Francisco, CA\", 50),\n",
        "    (\"Tokyo, JP\", 30),\n",
        "]\n",
        "\n",
        "for city, radius in geo_demos:\n",
        "    results = find_tweets_near(city, radius_km=radius)\n",
        "    print(f\"ğŸ“ Tweets near {city} (within {radius} km):\\n\")\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"  {i}. @{r['author']} â€” {r['distance_km']} km away\")\n",
        "        print(f\"     \\\"{r['text'][:80]}...\\\"\")\n",
        "        print(f\"     ğŸ·ï¸ {r['hashtags']}  â¤ï¸ {r['likes']} likes\\n\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea56013c",
      "metadata": {
        "id": "ea56013c"
      },
      "source": [
        "### ğŸ—ºï¸ Which cities are nearest to each other?\n",
        "\n",
        "We can also compute **inter-city distances** directly in the graph to see how our Place nodes relate geographically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "907c41a7",
      "metadata": {
        "id": "907c41a7"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Compute pairwise distances between all cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with driver.session(database=\"neo4j\") as session:\n",
        "    result = session.run(\"\"\"\n",
        "        MATCH (a:Place), (b:Place)\n",
        "        WHERE a.name < b.name  // avoid duplicates\n",
        "        RETURN a.name AS city_a,\n",
        "               b.name AS city_b,\n",
        "               round(point.distance(a.location, b.location) / 1000.0, 0) AS distance_km\n",
        "        ORDER BY distance_km ASC\n",
        "    \"\"\")\n",
        "    print(\"â”€â”€ City-to-City Distances â”€â”€\\n\")\n",
        "    for record in result:\n",
        "        print(f\"  {record['city_a']:>20s}  â†”  {record['city_b']:<20s}  {record['distance_km']:>8.0f} km\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e027c0",
      "metadata": {
        "id": "16e027c0"
      },
      "source": [
        "### ğŸ§  Geo-Augmented GraphRAG\n",
        "\n",
        "Combine **geospatial filtering + vector search + graph traversal** into a single retrieval function. This answers questions like:\n",
        "> *\"What are people near London saying about AI?\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbabe48",
      "metadata": {
        "id": "afbabe48"
      },
      "outputs": [],
      "source": [
        "def geo_graph_rag_search(question: str, city_name: str, radius_km: int = 100, k: int = 5) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Geo-filtered GraphRAG: vector search restricted to tweets\n",
        "    within `radius_km` of a city, then enriched via graph traversal.\n",
        "    \"\"\"\n",
        "    q_embedding = get_embeddings([question])[0]\n",
        "\n",
        "    cypher = \"\"\"\n",
        "        // Get city center\n",
        "        MATCH (p:Place {name: $city})\n",
        "        WITH p.location AS center\n",
        "\n",
        "        // Vector search â€” find semantically similar tweets\n",
        "        CALL db.index.vector.queryNodes($index, $k_broad, $embedding)\n",
        "        YIELD node AS tweet, score\n",
        "\n",
        "        // Geo filter â€” keep only tweets within radius\n",
        "        WHERE point.distance(tweet.location, center) < $radius_m\n",
        "\n",
        "        // Graph traversal â€” enrich\n",
        "        MATCH (author:User)-[:POSTED]->(tweet)\n",
        "        OPTIONAL MATCH (tweet)-[:LOCATED_AT]->(place:Place)\n",
        "        OPTIONAL MATCH (tweet)-[:TAGGED_WITH]->(hashtag:Hashtag)\n",
        "\n",
        "        RETURN tweet.text                       AS text,\n",
        "               score,\n",
        "               author.username                  AS author,\n",
        "               author.followers                 AS author_followers,\n",
        "               place.name                       AS location,\n",
        "               place.country                    AS country,\n",
        "               collect(DISTINCT hashtag.name)   AS hashtags,\n",
        "               tweet.likes                      AS likes,\n",
        "               tweet.retweets                   AS retweets,\n",
        "               round(point.distance(tweet.location, center) / 1000.0, 2) AS distance_km\n",
        "        ORDER BY score DESC\n",
        "        LIMIT $k\n",
        "    \"\"\"\n",
        "    with driver.session(database=\"neo4j\") as session:\n",
        "        # Search a broader pool then geo-filter down\n",
        "        result = session.run(\n",
        "            cypher,\n",
        "            city=city_name,\n",
        "            radius_m=radius_km * 1000,\n",
        "            index=INDEX_NAME,\n",
        "            k_broad=k * 10,  # cast a wider net for vector search\n",
        "            k=k,\n",
        "            embedding=q_embedding,\n",
        "        )\n",
        "        return [dict(record) for record in result]\n",
        "\n",
        "\n",
        "def geo_graph_rag_answer(question: str, city_name: str, radius_km: int = 100, k: int = 5) -> str:\n",
        "    \"\"\"Geo-filtered GraphRAG + Gemini LLM.\"\"\"\n",
        "    results = geo_graph_rag_search(question, city_name, radius_km, k)\n",
        "    if not results:\n",
        "        return f\"No tweets found near {city_name} matching your question.\"\n",
        "\n",
        "    context_lines = []\n",
        "    for i, r in enumerate(results, 1):\n",
        "        context_lines.append(\n",
        "            f\"Tweet {i} (similarity={r['score']:.3f}, {r['distance_km']} km from {city_name}):\\n\"\n",
        "            f\"  Text: \\\"{r['text']}\\\"\\n\"\n",
        "            f\"  Author: @{r['author']} ({r['author_followers']} followers)\\n\"\n",
        "            f\"  Location: {r['location']}, {r['country']}\\n\"\n",
        "            f\"  Hashtags: {', '.join(r['hashtags'])}\\n\"\n",
        "            f\"  Engagement: {r['likes']} likes, {r['retweets']} retweets\"\n",
        "        )\n",
        "    context = \"\\n\\n\".join(context_lines)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=LLM_MODEL,\n",
        "        contents=f\"Context:\\n{context}\\n\\nQuestion: {question}\",\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=SYSTEM_PROMPT,\n",
        "            temperature=0.2,\n",
        "        ),\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "\n",
        "print(\"âœ… Geo-augmented GraphRAG ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa6b211",
      "metadata": {
        "id": "efa6b211"
      },
      "outputs": [],
      "source": [
        "# â”€â”€ Geo-scoped questions across multiple cities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "geo_rag_demos = [\n",
        "    (\"What are people saying about AI and machine learning?\", \"London, UK\", 100),\n",
        "    (\"What do people think about cloud computing and serverless?\", \"New York, NY\", 80),\n",
        "    (\"What do people think about cloud computing and serverless?\", \"San Francisco, CA\", 80),\n",
        "    (\"What do people think about cloud computing and serverless?\", \"Tokyo, JP\", 80),\n",
        "]\n",
        "\n",
        "for question, city, radius in geo_rag_demos:\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"â“ {question}\")\n",
        "    print(f\"ğŸ“ Within {radius} km of {city}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(geo_graph_rag_answer(question, city, radius_km=radius))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "416982ac",
      "metadata": {
        "id": "416982ac"
      },
      "source": [
        "---\n",
        "## ğŸ§¹ Cleanup\n",
        "\n",
        "Close the Neo4j driver when finished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8aed44",
      "metadata": {
        "id": "be8aed44"
      },
      "outputs": [],
      "source": [
        "driver.close()\n",
        "print(\"âœ… Neo4j connection closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c5e48a",
      "metadata": {
        "id": "25c5e48a"
      },
      "source": [
        "---\n",
        "## ğŸ Conclusion\n",
        "\n",
        "We built a complete **GraphRAG** pipeline from the social media knowledge graph:\n",
        "\n",
        "| Component | What it does |\n",
        "|-----------|-------------|\n",
        "| **Vector Embeddings** | Encode tweet text into 768-d vectors via Gemini |\n",
        "| **Neo4j Vector Index** | Fast cosine-similarity search over tweet embeddings |\n",
        "| **Graph Traversal** | Enrich results with author, location, hashtags, and related tweets |\n",
        "| **LLM Generation** | Produce grounded answers from the enriched context |\n",
        "| **Cypher Generation** | Let the LLM write graph queries for analytical questions |\n",
        "| **Geospatial Queries** | Filter tweets by proximity using `point.distance()` |\n",
        "| **Geo-Augmented GraphRAG** | Combine vector search + geo-filter + graph traversal |\n",
        "\n",
        "**Key Insight:** GraphRAG produces *richer, more accurate* answers than plain vector search because it leverages the **structural relationships** between entities â€” not just text similarity. Adding geospatial filtering lets you scope answers to a specific region.\n",
        "\n",
        "**Next Steps:**\n",
        "- Add **community detection** to find clusters of related users.\n",
        "- Implement **hybrid search** (keyword + vector + graph).\n",
        "- Build a **Streamlit app** for interactive graph exploration.\n",
        "- Add **temporal analysis** â€” how do topics evolve over time?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}