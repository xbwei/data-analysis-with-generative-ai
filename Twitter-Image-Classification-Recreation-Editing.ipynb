{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c25a1b-a4e0-41fe-8cef-9e03203125f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Twitter Image Classification, Recreation, and Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f253-5662-4b4e-a6dd-9612ce1d881c",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook uses vision-language models to classify Twitter images and diffusion models to recreate and edit them.\n",
    "\n",
    "If you are using AWS SageMaker, ensure you are using the [Conda PyTorch kernel](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) that is specifically optimized for PyTorch-based deep learning tasks.\n",
    "\n",
    "Please check [LBSocial](www.lbsocial.net)  on how to collect Twitter data. \n",
    "\n",
    "\n",
    "## Visoan-Language Model\n",
    "A [Vision-Language Model (VLM)](https://huggingface.co/blog/vlms) is an AI model that integrates visual and textual information, enabling it to understand and generate insights from both images and text. It combines computer vision (to recognize objects and scenes) with natural language processing, allowing tasks like generating image captions, answering questions about images, and creating visuals from text prompts. \n",
    "\n",
    "## Diffusion Model\n",
    "A [diffusion model](https://huggingface.co/blog/Esmail-AGumaan/diffusion-models) is a generative AI model that creates images by reversing a process of adding noise. It learns to transform random noise into detailed images by progressively removing noise over many steps. Starting with pure noise, the model gradually refines each step until it forms a complete, high-quality image. This approach is highly effective for text-to-image generation and image editing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61eb4d-965a-474c-bc2e-c5bcc4da2526",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up a Database and API Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d5bea-cf3e-4eb8-ad23-0a128f979e65",
   "metadata": {},
   "source": [
    "Create a [MongoDB](www.mongodb.com) cluster and store the connection string in a safe place, such as AWS Secrets Manager. \n",
    "- key name: `connection_string`\n",
    "- key value: <`the connection string`>, you need to type the password\n",
    "- secret name: `mongodb`\n",
    "\n",
    "\n",
    "You also need to purchase and your [oepnai](https://openai.com/) api key in AWS Secrets Manager:\n",
    "- key name: `api_key`\n",
    "- key value: <`your openai api key`>\n",
    "- secret name: `openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67194e-60e9-44f2-a800-1da75b809fae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python Libraries\n",
    "\n",
    "- pymongo: manage the MongoDB database\n",
    "- openai: use the VLM model and diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541cd55-a63a-4ea7-b109-64e638f68058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install pymongo openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75f24d-4e08-471f-8ab2-5e1c062eabe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Secrets Manager Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9219e77-f395-4ec7-a81c-12d02c91c889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "    return json.loads(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6ed4e-b00d-4bdc-8465-81acbda700c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Python Libraries and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb73366-d206-417a-b34d-38ef4f9d3658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "openai_api_key  = get_secret('openai')['api_key']\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "\n",
    "mongodb_connect = get_secret('mongodb')['connection_string']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a33234-b6ec-48ea-9260-5796568b2648",
   "metadata": {},
   "source": [
    "## Connect to the MongoDB cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69718bb7-74d4-40a4-a970-34f7973e1321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mongo_client = MongoClient(mongodb_connect)\n",
    "db = mongo_client.demo # use or create a database named demo\n",
    "tweet_collection = db.tweet_collection #use or create a collection named tweet_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf88c5-ce0a-4663-9975-808baf3ec3be",
   "metadata": {},
   "source": [
    "## Extract Twitter Data\n",
    "Filter the Tweets you are interested in. You can use MongoDB Compass to help you write the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118129f5-8cdc-4dcf-a4ce-370b39a9eece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter={\n",
    "    'tweet.entities.urls.images': {\n",
    "        '$exists': True\n",
    "    }\n",
    "}\n",
    "project={\n",
    "    'tweet.entities.urls.images': 1, \n",
    "    'tweet.id': 1\n",
    "}\n",
    "result = tweet_collection.find(\n",
    "  filter=filter,\n",
    "  projection=project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9cd40-acbf-44a5-b6ab-ff213330dab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "for tweet in result:\n",
    "    for url in tweet['tweet']['entities']['urls']:\n",
    "        if 'images' in url:\n",
    "            tweet_data.append({'tweet_id':tweet['tweet']['id'],\n",
    "                               'image_url':url['images'][1]['url'] # 150*150 image \n",
    "                             })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a7297-a966-41f9-bebc-6338737d23f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Number of images: ',len(tweet_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05d66d-bbdc-4457-8a21-320f2d73a2fb",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "- `get_image_from_url`: Retrieve the image object from a URL.\n",
    "- `display_image`: Display an image in Python.\n",
    "- `image_to_bytes`: Convert an image to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eff974-2fcb-43ab-96d3-afee9eb79baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "def get_image_from_url(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    # print(response)\n",
    "    if response.status_code == 200 :\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    else:\n",
    "        print('image not accessible')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c6fc9-17ee-4727-9aff-12eb145c61a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(image):\n",
    "    if image:\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ded5ec-ca95-4a23-bd6a-9aa9a1a10890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def image_to_bytes(image_obj):\n",
    "    buffered = BytesIO()\n",
    "    image_obj.save(buffered, format=\"PNG\")\n",
    "    return buffered.getvalue()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68202e14-6cc3-428c-bac2-cf0cff4ad328",
   "metadata": {},
   "source": [
    "## Classify Twitter Image\n",
    "\n",
    "Many OpenAI [models](https://platform.openai.com/docs/models) can handle both images and text, allowing you to choose the most suitable model for your needs.\n",
    "\n",
    "This function summarizes each Twitter image and extracts entities into a JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc02d4-7607-47e2-82b7-c0a0548c803b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vison_model = 'gpt-4o-mini'\n",
    "temperature=0\n",
    "\n",
    "def describe_image(image_url):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=vison_model,\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"\"\"Analyze the image included in the tweet.\n",
    "                                        Briefly describe the content of the image and extract entities from it.\n",
    "                                        Organize the response in a JSON document:\n",
    "                                        Place the description in the <img_desc> key.\n",
    "                                        List the entity type and entity value in the <img_entities> key.\n",
    "                                        Do not wrap the JSON code in JSON markers.\"\"\"},\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\"url\": image_url,},\n",
    " \n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "\n",
    "      ],\n",
    "    temperature=temperature\n",
    "     )\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f898-fc00-4583-be35-5a020bf4e972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for tweet in tqdm(tweet_data):\n",
    "    \n",
    "    try:\n",
    "        image_result =json.loads(describe_image(tweet['image_url']))\n",
    "        tweet['image_desc']=image_result['img_desc']\n",
    "        tweet_collection.update_one(\n",
    "                {'tweet.id':tweet['tweet_id']},\n",
    "                {\"$set\":{'tweet.img_desc':image_result['img_desc'], \n",
    "                        'tweet.img_entities':image_result['img_entities']}}\n",
    "        )\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa623de-cf61-42ec-8a36-7af7c58a28ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize the extracted items\n",
    "\n",
    "- Apply the filter: `{'tweet.img_desc': {$ne: null}}` to ensure only tweets with image descriptions are included.\n",
    "- Heatmap: Create a heatmap to show the relationship between the entity type and entity name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ffc3d-6307-40d7-8f59-46a166acba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Load tweets with image descriptions\n",
    "tweets = list(tweet_collection.find({'tweet.img_desc': {'$ne': None}}))\n",
    "\n",
    "# Initialize containers\n",
    "entity_pairs = []\n",
    "tweet_data_with_desc = []\n",
    "\n",
    "# Process tweets\n",
    "for tweet in tweets:\n",
    "    tweet_obj = tweet.get('tweet', {})\n",
    "    tweet_id = tweet_obj.get('id')\n",
    "    img_desc = tweet_obj.get('img_desc')\n",
    "\n",
    "    # Extract image URL if available\n",
    "    for url in tweet_obj.get('entities', {}).get('urls', []):\n",
    "        if isinstance(url, dict) and 'images' in url and isinstance(url['images'], list) and len(url['images']) > 1:\n",
    "            tweet_data_with_desc.append({\n",
    "                'tweet_id': tweet_id,\n",
    "                'image_url': url['images'][1]['url'],\n",
    "                'image_desc': img_desc\n",
    "            })\n",
    "\n",
    "    # Extract entity pairs\n",
    "    entities = tweet_obj.get('img_entities', [])\n",
    "    if isinstance(entities, list):\n",
    "        for entity in entities:\n",
    "            etype = entity.get('entity_type')\n",
    "            evalue = entity.get('entity_value')\n",
    "            if isinstance(etype, str) and isinstance(evalue, str):\n",
    "                entity_pairs.append((etype, evalue))\n",
    "\n",
    "# Heatmap from top 10 (type, value) pairs\n",
    "top_10_pairs = Counter(entity_pairs).most_common(10)\n",
    "top_df = pd.DataFrame(top_10_pairs, columns=['Pair', 'Count'])\n",
    "top_df[['Entity Type', 'Entity Value']] = pd.DataFrame(top_df['Pair'].tolist(), index=top_df.index)\n",
    "heatmap_data = top_df.pivot(index='Entity Type', columns='Entity Value', values='Count').fillna(0)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, cmap='Blues', annot=True, fmt='g', linewidths=0.5)\n",
    "plt.title('Heatmap of Top 10 Entity Type vs. Entity Value Pairs', fontsize=14)\n",
    "plt.xlabel('Entity Value', fontsize=12)\n",
    "plt.ylabel('Entity Type', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb0137-6715-4a8c-a806-e9e9fcb85680",
   "metadata": {},
   "source": [
    "### View the image and description\n",
    "Randomly select an image, display it along with its description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766d4fb-adf2-488a-95a0-8d4bacce773f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet =random.choice(tweet_data_with_desc)\n",
    "print(tweet['image_url'])\n",
    "\n",
    "display_image(get_image_from_url(tweet['image_url']))\n",
    "\n",
    "print(tweet['image_desc'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547e15d-ae4d-4646-bfb5-36f0c37cd9a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Images\n",
    "For image generation, you can choose between DALL·E 2 or DALL·E 3. Both models currently support different parameters. Note that only DALL·E 2 can be used to create variations or edit images. Please check the OpenAI [API documentation](https://platform.openai.com/docs/guides/images) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b680f4-c774-4556-a421-36b2c2e760f7",
   "metadata": {},
   "source": [
    "### Create New Image\n",
    "Generate a new image based on the provided image description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80269afc-6fc6-4298-83c2-351c1c0d5a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_model=\"dall-e-2\"  \n",
    "\n",
    "\n",
    "def create_new_img(prompt):\n",
    "\n",
    "    resposne = client.images.generate(\n",
    "    model=image_model,\n",
    "    prompt=prompt,\n",
    "    n=1,\n",
    "    size = \"256x256\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(resposne.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b553d9-d980-4bee-ae20-5f2f597bd882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_image = create_new_img(tweet['image_desc'])\n",
    "\n",
    "display_image(get_image_from_url(new_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb289f14-1768-4c28-8922-315850c48095",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Variant\n",
    "Upload an existing Twitter image and create a variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ebab0-3a9c-4383-9ff9-ade79f14a7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_var_img(image_url):\n",
    "    img_obj = get_image_from_url(image_url)\n",
    "    img_byt = image_to_bytes(img_obj)\n",
    "    response = client.images.create_variation(\n",
    "        image=img_byt,\n",
    "        n=1,\n",
    "        size=\"256x256\",\n",
    "\n",
    "        )\n",
    "    return (response.data[0].url)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e64856-95cd-45aa-b67c-f69f1f758729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_image = create_var_img(tweet['image_url'])\n",
    "\n",
    "display_image(get_image_from_url(var_image))\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b001719-eb90-4c18-814c-c24453ab96e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Edit Image\n",
    "Upload Twitter images, mask the desired area, and edit the masked part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbebd08-c953-4027-adba-b5c005f2a937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_img_edit(image, img_mask, prompt):\n",
    "\n",
    "    response = client.images.edit(\n",
    "      model=\"dall-e-2\",\n",
    "      image=image_to_bytes(image),\n",
    "      mask=image_to_bytes(img_mask),\n",
    "      prompt=prompt,\n",
    "      n=1,\n",
    "      size=\"256x256\"\n",
    "    )\n",
    "\n",
    "    return (response.data[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c361fc9-85c0-476f-a577-c443104f8a5b",
   "metadata": {},
   "source": [
    "### Create Mask with PyTorch\n",
    "The following code is provided by ChatGPT with the prompt `\"Segment image and create masks.\"` \n",
    "\n",
    "You can also use foundational models to segment images, such as Meta’s [SAM (Segment Anything Model)](https://ai.meta.com/sam2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cad51-db31-4de1-8b57-a9f3f4d867d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "# Load the image\n",
    "image = get_image_from_url(tweet['image_url'])  # Replace with your image loading method\n",
    "\n",
    "# Define transformations (no resizing since images are 150x150)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained DeepLabV3 model\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Perform segmentation\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "\n",
    "# Convert output to predicted classes\n",
    "output_predictions = output.argmax(0).byte().numpy()\n",
    "\n",
    "\n",
    "# Create a transparent RGBA mask with fully transparent non-segmented areas\n",
    "height, width = output_predictions.shape\n",
    "mask = np.zeros((height, width, 4), dtype=np.uint8)  # 4 channels for RGBA\n",
    "\n",
    "# Define a color for segmented areas (e.g., semi-transparent red)\n",
    "color = [255, 0, 0, 180]  # Red with 70% opacity\n",
    "mask[output_predictions > 0] = color  # Apply color to segmented areas only\n",
    "\n",
    "# Convert mask to a PIL Image\n",
    "mask_image = Image.fromarray(mask, mode=\"RGBA\")\n",
    "\n",
    "# Overlay mask onto the contrast-enhanced original image\n",
    "image_with_mask = Image.alpha_composite(image.convert(\"RGBA\"), mask_image)\n",
    "\n",
    "# Display the original image and transparent mask overlay\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\" Image\")\n",
    "\n",
    "axs[1].imshow(image_with_mask)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Image Transparent Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4d045-4600-4256-b7d0-e884db28f0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_edit = create_img_edit(image,mask_image,'in the middle of a lake with yellow ducks')\n",
    "\n",
    "display_image(\n",
    "    get_image_from_url(img_edit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865863f2-15be-4984-bedb-f1a6626704f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Respoisbile AI\n",
    "Many image model providers update their security measures to foster responsible AI usage. For example, AWS adds watermarks to images generated by Amazon Titan. OpenAI has updated its safety features, including monitoring for abuse, access control, and the identification of images created by DALL·E 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7f519-eda0-4def-b321-bf9fdeba6968",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Amazon Web Services, Inc. *“Watermark Detection for Amazon Titan Image Generator Now Available in Amazon Bedrock.”* Accessed November 7, 2024. [https://aws.amazon.com/about-aws/whats-new/2024/04/watermark-detection-amazon-titan-image-generator-bedrock/](https://aws.amazon.com/about-aws/whats-new/2024/04/watermark-detection-amazon-titan-image-generator-bedrock/).\n",
    "\n",
    "- Esmail Atta Gumaan. *“Diffusion Models.”* Hugging Face (blog). Accessed November 7, 2024. https://huggingface.co/blog/Esmail-AGumaan/diffusion-models.\n",
    "\n",
    "- Merve Noyan and Edward Beeching. *“Vision Language Models Explained.”* Hugging Face (blog). Accessed November 6, 2024. [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n",
    "\n",
    "- *“OpenAI Safety Practices.”* Accessed November 7, 2024. [https://openai.com/index/openai-safety-update/](https://openai.com/index/openai-safety-update/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd95c31-9122-443f-bce7-de15ec8cca5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
