{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44c25a1b-a4e0-41fe-8cef-9e03203125f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Intro to Vison and Video Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f253-5662-4b4e-a6dd-9612ce1d881c",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook uses vision-language models to classify images and diffusion models to recreate and edit them.\n",
    "\n",
    "If you are using AWS SageMaker, ensure you are using the [Conda PyTorch kernel](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-pytorch.html) that is specifically optimized for PyTorch-based deep learning tasks.\n",
    "\n",
    "\n",
    "## Vision-Language Model\n",
    "A [Vision-Language Model (VLM)](https://huggingface.co/blog/vlms) is an AI model that integrates visual and textual information, enabling it to understand and generate insights from both images and text. It combines computer vision (to recognize objects and scenes) with natural language processing, allowing tasks like generating image captions, answering questions about images, and creating visuals from text prompts. \n",
    "\n",
    "## Diffusion Model\n",
    "A [diffusion model](https://huggingface.co/blog/Esmail-AGumaan/diffusion-models) is a generative AI model that creates images by reversing a process of adding noise. It learns to transform random noise into detailed images by progressively removing noise over many steps. Starting with pure noise, the model gradually refines each step until it forms a complete, high-quality image. This approach is highly effective for text-to-image generation and image editing.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61eb4d-965a-474c-bc2e-c5bcc4da2526",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d5bea-cf3e-4eb8-ad23-0a128f979e65",
   "metadata": {},
   "source": [
    "\n",
    "You also need to purchase and your [openai](https://openai.com/) api key in AWS Secrets Manager:\n",
    "- key name: `api_key`\n",
    "- key value: <`your openai api key`>\n",
    "- secret name: `openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67194e-60e9-44f2-a800-1da75b809fae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Python Libraries\n",
    "\n",
    "- pymongo: manage the MongoDB database\n",
    "- openai: use the VLM model and diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541cd55-a63a-4ea7-b109-64e638f68058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install openai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75f24d-4e08-471f-8ab2-5e1c062eabe3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Secrets Manager Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9219e77-f395-4ec7-a81c-12d02c91c889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    \n",
    "    return json.loads(secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6ed4e-b00d-4bdc-8465-81acbda700c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Python Libraries and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb73366-d206-417a-b34d-38ef4f9d3658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "openai_api_key  = get_secret('openai')['api_key']\n",
    "client = OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05d66d-bbdc-4457-8a21-320f2d73a2fb",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "- `get_image_from_url`: Retrieve the image object from a URL.\n",
    "- `display_image`: Display an image in Python.\n",
    "- `image_to_bytes`: Convert an image to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eff974-2fcb-43ab-96d3-afee9eb79baa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "def get_image_from_url(image_url):\n",
    "    response = requests.get(image_url)\n",
    "    # print(response)\n",
    "    if response.status_code == 200 :\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        return image\n",
    "    else:\n",
    "        print('image not accessible')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c6fc9-17ee-4727-9aff-12eb145c61a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image(image):\n",
    "    if image:\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ded5ec-ca95-4a23-bd6a-9aa9a1a10890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def image_to_bytes(image_obj):\n",
    "    buffer = BytesIO()\n",
    "    image_obj.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)  # move to beginning so OpenAI can read it\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712803c9-697b-4694-aa18-0468afdd4262",
   "metadata": {},
   "source": [
    "# Image URL\n",
    "Find an online image and past the URL here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc87f7-694f-4b93-8adf-a33f419a6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://www.jmu.edu/_images/news/2025/xuebin-wei-textbook1.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833af2b-c511-4beb-83ac-8acf448e0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(get_image_from_url(image_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68202e14-6cc3-428c-bac2-cf0cff4ad328",
   "metadata": {},
   "source": [
    "## Classify Image\n",
    "\n",
    "Many OpenAI [models](https://platform.openai.com/docs/models) can handle both images and text, allowing you to choose the most suitable model for your needs.\n",
    "\n",
    "This function summarizes each Twitter image and extracts entities into a JSON document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc02d4-7607-47e2-82b7-c0a0548c803b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vison_model = 'gpt-4o-mini'\n",
    "temperature=0\n",
    "\n",
    "def describe_image(image_url):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=vison_model,\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"\"\" Analyze the image included in the tweet.\n",
    "                                        Briefly describe the content of the image and extract entities from it.\n",
    "                                        Organize the response in a JSON document:\n",
    "                                        Place the description in the <img_desc> key.\n",
    "                                        List the entity type and entity value in the <img_entities> key.\n",
    "                                        Do not wrap the JSON code in JSON markers.\"\"\"},\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\"url\": image_url,},\n",
    " \n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "\n",
    "      ],\n",
    "    temperature=temperature\n",
    "     )\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f898-fc00-4583-be35-5a020bf4e972",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_result = json.loads(describe_image(image_url))\n",
    "pprint(image_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547e15d-ae4d-4646-bfb5-36f0c37cd9a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Images\n",
    "For image generation, you can choose between DALL·E 2 or DALL·E 3. Both models currently support different parameters. Note that only DALL·E 2 can be used to create variations or edit images. Please check the OpenAI [API documentation](https://platform.openai.com/docs/guides/images) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b680f4-c774-4556-a421-36b2c2e760f7",
   "metadata": {},
   "source": [
    "### Create New Image\n",
    "Generate a new image based on the provided image description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80269afc-6fc6-4298-83c2-351c1c0d5a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_model=\"dall-e-2\"  \n",
    "\n",
    "\n",
    "def create_new_img(prompt):\n",
    "\n",
    "    resposne = client.images.generate(\n",
    "    model=image_model,\n",
    "    prompt=prompt,\n",
    "    n=1,\n",
    "    size = \"256x256\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return(resposne.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b553d9-d980-4bee-ae20-5f2f597bd882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_image = create_new_img(image_result['img_desc'])\n",
    "\n",
    "display_image(get_image_from_url(new_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb289f14-1768-4c28-8922-315850c48095",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Variant\n",
    "Upload an existing Twitter image and create a variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ebab0-3a9c-4383-9ff9-ade79f14a7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_var_img(image_url):\n",
    "    img_obj = get_image_from_url(image_url)\n",
    "    img_byt = image_to_bytes(img_obj)\n",
    "    response = client.images.create_variation(\n",
    "        image=img_byt,\n",
    "        n=1,\n",
    "        size=\"256x256\",\n",
    "\n",
    "        )\n",
    "    return (response.data[0].url)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e64856-95cd-45aa-b67c-f69f1f758729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_image = create_var_img(image_url)\n",
    "\n",
    "display_image(get_image_from_url(var_image))\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b001719-eb90-4c18-814c-c24453ab96e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Edit Image\n",
    "Upload Twitter images, mask the desired area, and edit the masked part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbebd08-c953-4027-adba-b5c005f2a937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "def create_img_edit(image, img_mask, prompt):\n",
    "    \"\"\"Edit an image using DALL·E 2 or GPT-Image-1 with proper PNG encoding.\"\"\"\n",
    "    # Convert base image to PNG bytes\n",
    "    base_buffer = BytesIO()\n",
    "    image.convert(\"RGBA\").save(base_buffer, format=\"PNG\")\n",
    "    base_buffer.seek(0)\n",
    "\n",
    "    # Convert mask to PNG bytes\n",
    "    mask_buffer = BytesIO()\n",
    "    img_mask.convert(\"RGBA\").save(mask_buffer, format=\"PNG\")\n",
    "    mask_buffer.seek(0)\n",
    "\n",
    "    response = client.images.edit(\n",
    "        model=\"dall-e-2\",  # or \"gpt-image-1\"\n",
    "        image=(\"base.png\", base_buffer, \"image/png\"),\n",
    "        mask=(\"mask.png\", mask_buffer, \"image/png\"),\n",
    "        prompt=prompt,\n",
    "        n=1,\n",
    "        size=\"256x256\"\n",
    "    )\n",
    "\n",
    "    return response.data[0].url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c361fc9-85c0-476f-a577-c443104f8a5b",
   "metadata": {},
   "source": [
    "### Create Mask with PyTorch\n",
    "The following code is provided by ChatGPT with the prompt `\"Segment image and create masks.\"` \n",
    "\n",
    "You can also use foundational models to segment images, such as Meta’s [SAM (Segment Anything Model)](https://ai.meta.com/sam2/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cad51-db31-4de1-8b57-a9f3f4d867d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "# Load the image\n",
    "image = get_image_from_url(image_url)  # Replace with your image loading method\n",
    "\n",
    "# Define transformations (no resizing since images are 150x150)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "input_image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Load the pre-trained DeepLabV3 model\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Perform segmentation\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "\n",
    "# Convert output to predicted classes\n",
    "output_predictions = output.argmax(0).byte().numpy()\n",
    "\n",
    "\n",
    "# Create a transparent RGBA mask with fully transparent non-segmented areas\n",
    "height, width = output_predictions.shape\n",
    "mask = np.zeros((height, width, 4), dtype=np.uint8)  # 4 channels for RGBA\n",
    "\n",
    "# Define a color for segmented areas (e.g., semi-transparent red)\n",
    "color = [255, 0, 0, 180]  # Red with 70% opacity\n",
    "mask[output_predictions > 0] = color  # Apply color to segmented areas only\n",
    "\n",
    "# Convert mask to a PIL Image\n",
    "mask_image = Image.fromarray(mask, mode=\"RGBA\")\n",
    "\n",
    "# Overlay mask onto the contrast-enhanced original image\n",
    "image_with_mask = Image.alpha_composite(image.convert(\"RGBA\"), mask_image)\n",
    "\n",
    "# Display the original image and transparent mask overlay\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].imshow(image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(\" Image\")\n",
    "\n",
    "axs[1].imshow(image_with_mask)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(\"Image Transparent Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4d045-4600-4256-b7d0-e884db28f0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_edit = create_img_edit(image,mask_image,'in the middle of a lake with yellow ducks')\n",
    "\n",
    "display_image(\n",
    "    get_image_from_url(img_edit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f87c82-0b23-400a-9cf2-ede030dffa9b",
   "metadata": {},
   "source": [
    "# Create videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43b0cf-10f7-4975-a47a-a2e35723a3b3",
   "metadata": {},
   "source": [
    "Create video with a pommpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55a221-6f53-4ed8-a151-2d65b4a94b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video(prompt):\n",
    "\n",
    "    video = client.videos.create(\n",
    "\n",
    "        prompt=prompt,\n",
    " \n",
    "        )\n",
    "    return(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ed3d7-2d16-41af-9b0c-3e74b8cda70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = create_video('a person is eating a book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e83263-f326-4324-908a-6295ab1e6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc99bf6-16a5-4da9-92c4-045ba0a64382",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = client.videos.list()\n",
    "page = page.data\n",
    "pprint(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83998054-3d58-41e9-ba03-f6bab965c771",
   "metadata": {},
   "source": [
    "Waite untile your video id appears in the following list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee529834-ffbb-47ae-ae08-4bfbef28130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.videos.download_content(\n",
    "    video_id=video.id,\n",
    ")\n",
    "content = response.read()\n",
    "\n",
    "# Save to file\n",
    "output_path = f\"generated_video_{video.id}.mp4\"\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(f\"Video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865863f2-15be-4984-bedb-f1a6626704f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Respoisbile AI\n",
    "Many image model providers update their security measures to foster responsible AI usage. For example, AWS adds watermarks to images generated by Amazon Titan. OpenAI has updated its safety features, including monitoring for abuse, access control, and the identification of images created by DALL·E 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7f519-eda0-4def-b321-bf9fdeba6968",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Amazon Web Services, Inc. *“Watermark Detection for Amazon Titan Image Generator Now Available in Amazon Bedrock.”* Accessed November 7, 2024. [https://aws.amazon.com/about-aws/whats-new/2024/04/watermark-detection-amazon-titan-image-generator-bedrock/](https://aws.amazon.com/about-aws/whats-new/2024/04/watermark-detection-amazon-titan-image-generator-bedrock/).\n",
    "\n",
    "- Esmail Atta Gumaan. *“Diffusion Models.”* Hugging Face (blog). Accessed November 7, 2024. https://huggingface.co/blog/Esmail-AGumaan/diffusion-models.\n",
    "\n",
    "- Merve Noyan and Edward Beeching. *“Vision Language Models Explained.”* Hugging Face (blog). Accessed November 6, 2024. [https://huggingface.co/blog/vlms](https://huggingface.co/blog/vlms).\n",
    "\n",
    "- *“OpenAI Safety Practices.”* Accessed November 7, 2024. [https://openai.com/index/openai-safety-update/](https://openai.com/index/openai-safety-update/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd95c31-9122-443f-bce7-de15ec8cca5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988bc5a-d502-4866-89af-8e083f405595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be97873-aeee-4332-a617-e79a28aa8cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
